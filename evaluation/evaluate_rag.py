"""
RAGAS Evaluation for Legal RAG System

Evaluates RAG performance using RAGAS metrics:
- Faithfulness: Answer faithful to retrieved context
- Answer Relevancy: Answer relevant to question
- Context Precision: Retrieved context quality
- Context Recall: Coverage of ground truth
- Context Relevancy: Context relevant to question

For Vietnamese legal documents

Usage:
    # Run with default test cases (from evaluation/ directory)
    python evaluation/evaluate_rag.py

    # Or from project root
    python -m evaluation.evaluate_rag

    # Run with custom test dataset
    python evaluation/evaluate_rag.py --dataset evaluation/test_dataset.json

    # Run specific document IDs only
    python evaluation/evaluate_rag.py --document-ids 1,2,3

    # Disable reranking
    python evaluation/evaluate_rag.py --no-reranking

    # Save detailed results
    python evaluation/evaluate_rag.py --output results/my_results.json

    # Skip RAGAS evaluation (only legal metrics)
    python evaluation/evaluate_rag.py --llm-provider none
"""
import argparse
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from datasets import Dataset

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import settings after adding project root to path
from config.settings import settings

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_evaluation_dataset(test_cases: List[Dict[str, Any]]) -> Dataset:
    """
    Create RAGAS evaluation dataset

    Args:
        test_cases: List of test cases with:
            - question: User question
            - ground_truth: Expected answer (optional for some metrics)
            - contexts: Retrieved contexts (will be filled by RAG)
            - answer: Generated answer (will be filled by RAG)

    Returns:
        HuggingFace Dataset for RAGAS
    """
    return Dataset.from_pandas(pd.DataFrame(test_cases))


def run_rag_evaluation(
    rag_engine,
    test_questions: List[str],
    ground_truths: List[str] = None,
    document_ids: List[int] = None
) -> Dict[str, Any]:
    """
    Run RAG system and collect results for evaluation

    Args:
        rag_engine: RAGEngine instance
        test_questions: List of test questions
        ground_truths: Optional list of ground truth answers
        document_ids: Optional document IDs to filter

    Returns:
        Dict with evaluation data
    """
    results = []

    for i, question in enumerate(test_questions):
        logger.info(f"Processing question {i+1}/{len(test_questions)}: {question[:50]}...")

        # Query RAG
        response = rag_engine.query(
            question=question,
            document_ids=document_ids,
            top_k=5
        )

        # Extract contexts from sources
        contexts = [
            source.get('content', '')
            for source in response.get('sources', [])
        ]

        result = {
            'question': question,
            'answer': response.get('answer', ''),
            'contexts': contexts,
        }

        # Add ground truth if available
        if ground_truths and i < len(ground_truths):
            result['ground_truth'] = ground_truths[i]

        results.append(result)

    return results


def evaluate_with_ragas(
    evaluation_data: List[Dict[str, Any]],
    llm_provider: str = "fpt"  # "openai", "fpt", or "local"
) -> Dict[str, float]:
    """
    Evaluate RAG using RAGAS metrics

    Uses configuration from config/settings.py for all evaluation parameters.

    Args:
        evaluation_data: List of dicts with question, answer, contexts, ground_truth
        llm_provider: LLM provider ("openai", "fpt", or "local")

    Returns:
        Dict of metric scores
    """
    try:
        import os
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )
        from config.settings import settings

        # Create dataset
        dataset = create_evaluation_dataset(evaluation_data)

        # Select metrics based on available data
        metrics = [faithfulness, answer_relevancy]

        # Only add precision/recall if ground_truth is available
        if 'ground_truth' in evaluation_data[0]:
            metrics.extend([context_precision, context_recall])

        # Configure LLM based on provider
        llm = None
        embeddings = None

        if llm_provider == "fpt":
            # FPT Cloud configuration from settings
            from langchain_openai import ChatOpenAI, OpenAIEmbeddings

            # Get API key from settings (reads from env: EVAL_LLM_API_KEY -> FPT_API_KEY -> OPENAI_API_KEY)
            api_key = settings.EVAL_LLM_API_KEY

            if not api_key:
                logger.error("No API key found. Set EVAL_LLM_API_KEY, FPT_API_KEY, or OPENAI_API_KEY environment variable")
                return {}

            # LLM configuration
            llm = ChatOpenAI(
                model=settings.EVAL_LLM_MODEL,
                api_key=api_key,
                base_url=settings.EVAL_LLM_BASE_URL,
                temperature=0.1,
                max_retries=settings.EVAL_LLM_MAX_RETRIES,
                request_timeout=settings.EVAL_LLM_TIMEOUT
            )

            # Embeddings configuration (from settings with fallback chain)
            embedding_api_key = settings.EVAL_EMBEDDING_API_KEY
            embedding_base_url = settings.EVAL_EMBEDDING_BASE_URL

            embeddings = OpenAIEmbeddings(
                model=settings.EVAL_EMBEDDING_MODEL,
                api_key=embedding_api_key,
                base_url=embedding_base_url if embedding_base_url else None
            )

            logger.info(f"Using {settings.EVAL_LLM_MODEL} for evaluation")
            logger.info(f"  LLM endpoint: {settings.EVAL_LLM_BASE_URL}")
            if embedding_base_url != settings.EVAL_LLM_BASE_URL:
                logger.info(f"  Embedding endpoint: {embedding_base_url}")

        elif llm_provider == "local":
            # Local llama.cpp model
            from langchain_community.llms import LlamaCpp

            llm = LlamaCpp(
                model_path=str(settings.model_path),
                n_ctx=settings.LLM_CONTEXT_LENGTH,
                n_threads=settings.LLAMACPP_N_THREADS,
                temperature=settings.LLM_TEMPERATURE,
                verbose=False
            )

            logger.info(f"Using local model for evaluation: {settings.LLM_MODEL_NAME}")

        # If llm is configured, wrap it for RAGAS
        ragas_kwargs = {}
        if llm:
            from ragas.llms import LangChainLLMWrapper
            # Use bypass_n for custom endpoints (FPT, local)
            bypass_n = llm_provider in ["fpt", "local"]
            ragas_kwargs['llm'] = LangChainLLMWrapper(llm, bypass_n=bypass_n)

        if embeddings:
            from ragas.embeddings import LangchainEmbeddingsWrapper
            ragas_kwargs['embeddings'] = LangchainEmbeddingsWrapper(embeddings)

        logger.info(f"Running RAGAS evaluation with {len(metrics)} metrics...")

        # Run evaluation
        results = evaluate(dataset, metrics=metrics, **ragas_kwargs)

        # Convert to dict
        scores = {
            metric: float(results[metric])
            for metric in results.keys()
        }

        return scores

    except ImportError as e:
        logger.error(f"RAGAS not installed or missing dependencies: {e}")
        logger.error("Install with: pip install ragas langchain-openai")
        return {}
    except Exception as e:
        logger.error(f"Error during RAGAS evaluation: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {}


def print_evaluation_report(
    scores: Dict[str, float],
    evaluation_data: List[Dict[str, Any]],
    legal_scores: Dict[str, float] = None
):
    """
    Print formatted evaluation report

    Args:
        scores: RAGAS metric scores
        evaluation_data: Evaluation data with questions/answers
        legal_scores: Optional legal-specific metric scores
    """
    print("\n" + "="*80)
    print("RAGAS EVALUATION REPORT")
    print("="*80)

    # Overall scores
    print("\nğŸ“Š Overall Metrics:")
    print("-" * 80)
    for metric, score in scores.items():
        emoji = "âœ…" if score >= 0.7 else "âš ï¸" if score >= 0.5 else "âŒ"
        print(f"  {emoji} {metric.replace('_', ' ').title()}: {score:.3f}")

    # Average
    if scores:
        avg_score = sum(scores.values()) / len(scores)
        print(f"\n  ğŸ“ˆ Average Score: {avg_score:.3f}")

    # Legal-specific metrics
    if legal_scores:
        from legal_metrics import print_legal_metrics_report
        print_legal_metrics_report(legal_scores)

    # Sample results
    print("\nğŸ“ Sample Results:")
    print("-" * 80)
    for i, item in enumerate(evaluation_data[:3], 1):
        print(f"\n  Example {i}:")
        print(f"  Question: {item['question'][:100]}...")
        print(f"  Answer: {item['answer'][:150]}...")
        print(f"  Contexts: {len(item['contexts'])} chunks retrieved")
        if 'ground_truth' in item:
            print(f"  Ground Truth: {item['ground_truth'][:100]}...")
        if 'expected_articles' in item:
            print(f"  Expected Articles: {item['expected_articles']}")

    print("\n" + "="*80 + "\n")


def load_test_dataset(file_path: str) -> List[Dict[str, Any]]:
    """
    Load test cases from JSON file

    Args:
        file_path: Path to JSON file with test cases

    Returns:
        List of test cases
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        test_cases = data.get('test_cases', [])
        logger.info(f"Loaded {len(test_cases)} test cases from {file_path}")

        return test_cases

    except FileNotFoundError:
        logger.error(f"Test dataset file not found: {file_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in test dataset: {e}")
        sys.exit(1)


def validate_test_cases(test_cases: List[Dict[str, Any]]) -> bool:
    """
    Validate test cases have required fields

    Args:
        test_cases: List of test cases to validate

    Returns:
        True if valid, False otherwise
    """
    required_fields = ['question']

    for i, case in enumerate(test_cases, 1):
        missing = [field for field in required_fields if field not in case]
        if missing:
            logger.error(f"Test case {i} missing required fields: {missing}")
            return False

    return True


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Run RAGAS evaluation on Vietnamese Legal RAG system'
    )

    parser.add_argument(
        '--dataset',
        type=str,
        help='Path to test dataset JSON file'
    )

    parser.add_argument(
        '--document-ids',
        type=str,
        help='Comma-separated document IDs to filter (e.g., "1,2,3")'
    )

    parser.add_argument(
        '--no-reranking',
        action='store_true',
        help='Disable reranking in RAG engine'
    )

    parser.add_argument(
        '--top-k',
        type=int,
        default=5,
        help='Number of top results to retrieve (default: 5)'
    )

    parser.add_argument(
        '--output',
        type=str,
        default='evaluation_results.json',
        help='Output file for results (default: evaluation_results.json)'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )

    parser.add_argument(
        '--llm-provider',
        type=str,
        default='fpt',
        choices=['fpt', 'openai', 'local', 'none'],
        help='LLM provider for RAGAS evaluation (default: fpt). Use "none" to skip RAGAS.'
    )

    parser.add_argument(
        '--fpt-api-key',
        type=str,
        help='FPT Cloud API key (default: from FPT_API_KEY env variable)'
    )

    return parser.parse_args()


# Example test cases for Vietnamese legal documents
LEGAL_TEST_CASES = [
    # Luáº­t ÄÆ°á»ng sáº¯t (95/2025/QH15)
    {
        "question": "Luáº­t ÄÆ°á»ng sáº¯t quy Ä‘á»‹nh pháº¡m vi Ä‘iá»u chá»‰nh nhÆ° tháº¿ nÃ o?",
        "ground_truth": "Luáº­t ÄÆ°á»ng sáº¯t quy Ä‘á»‹nh vá» hoáº¡t Ä‘á»™ng Ä‘Æ°á»ng sáº¯t; quyá»n, nghÄ©a vá»¥ vÃ  trÃ¡ch nhiá»‡m cá»§a tá»• chá»©c, cÃ¡ nhÃ¢n liÃªn quan Ä‘áº¿n hoáº¡t Ä‘á»™ng Ä‘Æ°á»ng sáº¯t.",
        "expected_articles": [1],
        "category": "railway",
        "difficulty": "easy"
    },
    {
        "question": "CÃ¡c hÃ nh vi bá»‹ nghiÃªm cáº¥m trong hoáº¡t Ä‘á»™ng Ä‘Æ°á»ng sáº¯t lÃ  gÃ¬?",
        "ground_truth": "CÃ¡c hÃ nh vi bá»‹ nghiÃªm cáº¥m bao gá»“m: phÃ¡ hoáº¡i cÃ´ng trÃ¬nh Ä‘Æ°á»ng sáº¯t, phÆ°Æ¡ng tiá»‡n giao thÃ´ng Ä‘Æ°á»ng sáº¯t; láº¥n chiáº¿m hÃ nh lang an toÃ n; lÃ m sai lá»‡ch há»‡ thá»‘ng bÃ¡o hiá»‡u; tá»± Ã½ bÃ¡o hiá»‡u dá»«ng tÃ u; Ä‘á»ƒ chÆ°á»›ng ngáº¡i váº­t, cháº¥t dá»… chÃ¡y ná»• trong pháº¡m vi báº£o vá»‡; Ä‘iá»u khiá»ƒn tÃ u quÃ¡ tá»‘c Ä‘á»™; nhÃ¢n viÃªn Ä‘Æ°á»ng sáº¯t cÃ³ ná»“ng Ä‘á»™ cá»“n hoáº·c ma tÃºy trong ngÆ°á»i.",
        "expected_articles": [6],
        "category": "railway",
        "difficulty": "medium"
    },
    {
        "question": "ÄÆ°á»ng sáº¯t Viá»‡t Nam Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh nhá»¯ng loáº¡i nÃ o?",
        "ground_truth": "Há»‡ thá»‘ng Ä‘Æ°á»ng sáº¯t Viá»‡t Nam bao gá»“m: Ä‘Æ°á»ng sáº¯t quá»‘c gia (phá»¥c vá»¥ váº­n táº£i chung vÃ  liÃªn váº­n quá»‘c táº¿), Ä‘Æ°á»ng sáº¯t Ä‘á»‹a phÆ°Æ¡ng (phá»¥c vá»¥ nhu cáº§u váº­n táº£i cá»§a Ä‘á»‹a phÆ°Æ¡ng vÃ  vÃ¹ng kinh táº¿, bao gá»“m Ä‘Æ°á»ng sáº¯t Ä‘Ã´ thá»‹), vÃ  Ä‘Æ°á»ng sáº¯t chuyÃªn dÃ¹ng (phá»¥c vá»¥ nhu cáº§u riÃªng cá»§a tá»• chá»©c, cÃ¡ nhÃ¢n).",
        "expected_articles": [7],
        "category": "railway",
        "difficulty": "medium"
    },
    {
        "question": "Khá»• Ä‘Æ°á»ng sáº¯t tiÃªu chuáº©n vÃ  khá»• Ä‘Æ°á»ng háº¹p cÃ³ kÃ­ch thÆ°á»›c bao nhiÃªu?",
        "ground_truth": "Khá»• Ä‘Æ°á»ng sáº¯t tiÃªu chuáº©n lÃ  1435 mm vÃ  khá»• Ä‘Æ°á»ng háº¹p lÃ  1000 mm. ÄÆ°á»ng sáº¯t quá»‘c gia vÃ  Ä‘á»‹a phÆ°Æ¡ng Ä‘áº§u tÆ° má»›i pháº£i Ã¡p dá»¥ng khá»• Ä‘Æ°á»ng tiÃªu chuáº©n.",
        "expected_articles": [8],
        "category": "railway",
        "difficulty": "easy"
    },
    {
        "question": "Há»‡ thá»‘ng tÃ­n hiá»‡u giao thÃ´ng Ä‘Æ°á»ng sáº¯t bao gá»“m nhá»¯ng gÃ¬?",
        "ground_truth": "Há»‡ thá»‘ng tÃ­n hiá»‡u giao thÃ´ng Ä‘Æ°á»ng sáº¯t bao gá»“m: hiá»‡u lá»‡nh cá»§a ngÆ°á»i Ä‘iá»u khiá»ƒn cháº¡y tÃ u, há»‡ thá»‘ng Ä‘iá»u khiá»ƒn cháº¡y tÃ u, tÃ­n hiá»‡u trÃªn tÃ u, tÃ­n hiá»‡u dÆ°á»›i máº·t Ä‘áº¥t, biá»ƒn bÃ¡o hiá»‡u, phÃ¡o hiá»‡u phÃ²ng vá»‡, Ä‘uá»‘c. Há»‡ thá»‘ng nÃ y pháº£i Ä‘áº§y Ä‘á»§, chÃ­nh xÃ¡c, rÃµ rÃ ng Ä‘á»ƒ báº£o Ä‘áº£m an toÃ n.",
        "expected_articles": [11],
        "category": "railway",
        "difficulty": "hard"
    },

    # Luáº­t sá»­a Ä‘á»•i vá» QuÃ¢n sá»± (98/2025/QH15)
    {
        "question": "Khu vá»±c phÃ²ng thá»§ Ä‘Æ°á»£c tá»• chá»©c nhÆ° tháº¿ nÃ o theo Luáº­t Quá»‘c phÃ²ng?",
        "ground_truth": "Khu vá»±c phÃ²ng thá»§ lÃ  bá»™ pháº­n há»£p thÃ nh phÃ²ng thá»§ quÃ¢n khu, bao gá»“m cÃ¡c hoáº¡t Ä‘á»™ng vá» chÃ­nh trá»‹, tinh tháº§n, kinh táº¿, vÄƒn hÃ³a, xÃ£ há»™i, khoa há»c, cÃ´ng nghá»‡, quÃ¢n sá»±, an ninh, Ä‘á»‘i ngoáº¡i; Ä‘Æ°á»£c tá»• chá»©c theo Ä‘á»‹a bÃ n cáº¥p tá»‰nh, Ä‘Æ¡n vá»‹ hÃ nh chÃ­nh - kinh táº¿ Ä‘áº·c biá»‡t, láº¥y xÃ¢y dá»±ng phÃ²ng thá»§ khu vá»±c, xÃ¢y dá»±ng cáº¥p xÃ£ lÃ m ná»n táº£ng.",
        "expected_articles": [1, 9],
        "category": "military",
        "difficulty": "hard"
    },
    {
        "question": "Lá»‡nh thiáº¿t quÃ¢n luáº­t pháº£i xÃ¡c Ä‘á»‹nh nhá»¯ng ná»™i dung gÃ¬?",
        "ground_truth": "Lá»‡nh thiáº¿t quÃ¢n luáº­t pháº£i xÃ¡c Ä‘á»‹nh cá»¥ thá»ƒ Ä‘á»‹a phÆ°Æ¡ng cáº¥p tá»‰nh, cáº¥p xÃ£, Ä‘Æ¡n vá»‹ hÃ nh chÃ­nh - kinh táº¿ Ä‘áº·c biá»‡t thiáº¿t quÃ¢n luáº­t, biá»‡n phÃ¡p, hiá»‡u lá»±c thi hÃ nh; quy Ä‘á»‹nh nhiá»‡m vá»¥, quyá»n háº¡n cá»§a cÆ¡ quan, tá»• chá»©c, cÃ¡ nhÃ¢n; cÃ¡c quy táº¯c tráº­t tá»± xÃ£ há»™i cáº§n thiáº¿t vÃ  Ä‘Æ°á»£c cÃ´ng bá»‘ liÃªn tá»¥c trÃªn phÆ°Æ¡ng tiá»‡n thÃ´ng tin Ä‘áº¡i chÃºng.",
        "expected_articles": [1, 21],
        "category": "military",
        "difficulty": "medium"
    },
    {
        "question": "CÃ´ng dÃ¢n nam bao nhiÃªu tuá»•i pháº£i Ä‘Äƒng kÃ½ nghÄ©a vá»¥ quÃ¢n sá»± láº§n Ä‘áº§u?",
        "ground_truth": "CÃ´ng dÃ¢n nam Ä‘á»§ 17 tuá»•i trong nÄƒm pháº£i Ä‘Äƒng kÃ½ nghÄ©a vá»¥ quÃ¢n sá»± láº§n Ä‘áº§u. Viá»‡c Ä‘Äƒng kÃ½ Ä‘Æ°á»£c thá»±c hiá»‡n vÃ o thÃ¡ng tÆ° háº±ng nÄƒm, cÃ³ thá»ƒ báº±ng hÃ¬nh thá»©c trá»±c tuyáº¿n hoáº·c trá»±c tiáº¿p táº¡i cÆ¡ quan Ä‘Äƒng kÃ½ nghÄ©a vá»¥ quÃ¢n sá»±.",
        "expected_articles": [4, 16],
        "category": "military",
        "difficulty": "easy"
    },
    {
        "question": "HÃ nh vi trá»‘n trÃ¡nh nghÄ©a vá»¥ quÃ¢n sá»± Ä‘Æ°á»£c hiá»ƒu nhÆ° tháº¿ nÃ o?",
        "ground_truth": "Trá»‘n trÃ¡nh nghÄ©a vá»¥ quÃ¢n sá»± lÃ  hÃ nh vi khÃ´ng cháº¥p hÃ nh quyáº¿t Ä‘á»‹nh gá»i Ä‘Äƒng kÃ½ nghÄ©a vá»¥ quÃ¢n sá»±; quyáº¿t Ä‘á»‹nh gá»i khÃ¡m sá»©c khá»e nghÄ©a vá»¥ quÃ¢n sá»±; quyáº¿t Ä‘á»‹nh gá»i nháº­p ngÅ©; quyáº¿t Ä‘á»‹nh gá»i táº­p trung huáº¥n luyá»‡n, diá»…n táº­p, kiá»ƒm tra sáºµn sÃ ng Ä‘á»™ng viÃªn, sáºµn sÃ ng chiáº¿n Ä‘áº¥u.",
        "expected_articles": [4, 3],
        "category": "military",
        "difficulty": "medium"
    },
    {
        "question": "Há»™i Ä‘á»“ng nghÄ©a vá»¥ quÃ¢n sá»± cáº¥p tá»‰nh cÃ³ nhá»¯ng nhiá»‡m vá»¥ gÃ¬?",
        "ground_truth": "Há»™i Ä‘á»“ng nghÄ©a vá»¥ quÃ¢n sá»± cáº¥p tá»‰nh giÃºp UBND cáº¥p tá»‰nh: chá»‰ Ä‘áº¡o Ä‘Äƒng kÃ½ nghÄ©a vá»¥ quÃ¢n sá»± vÃ  quáº£n lÃ½ cÃ´ng dÃ¢n trong Ä‘á»™ tuá»•i; tuyá»ƒn chá»n gá»i cÃ´ng dÃ¢n nháº­p ngÅ©; bÃ¡o cÃ¡o quyáº¿t Ä‘á»‹nh cÃ´ng dÃ¢n Ä‘Æ°á»£c gá»i nháº­p ngÅ©, táº¡m hoÃ£n, miá»…n gá»i; chá»‰ Ä‘áº¡o UBND cáº¥p xÃ£; tá»• chá»©c bÃ n giao cÃ´ng dÃ¢n cho Ä‘Æ¡n vá»‹ quÃ¢n Ä‘á»™i; kiá»ƒm tra thá»±c hiá»‡n chÃ­nh sÃ¡ch háº­u phÆ°Æ¡ng quÃ¢n Ä‘á»™i; giáº£i quyáº¿t khiáº¿u náº¡i, tá»‘ cÃ¡o.",
        "expected_articles": [4, 37],
        "category": "military",
        "difficulty": "hard"
    },
]


def main():
    """Main evaluation workflow with CLI arguments"""
    import os
    from src.rag.rag_engine import RAGEngine
    from legal_metrics import evaluate_legal_metrics

    # Parse arguments
    args = parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Load test cases
    if args.dataset:
        test_cases = load_test_dataset(args.dataset)
    else:
        logger.info("Using default test cases from LEGAL_TEST_CASES")
        test_cases = LEGAL_TEST_CASES

    # Validate test cases
    if not validate_test_cases(test_cases):
        logger.error("Test case validation failed")
        sys.exit(1)

    # Parse document IDs
    document_ids = None
    if args.document_ids:
        try:
            document_ids = [int(id.strip()) for id in args.document_ids.split(',')]
            logger.info(f"Filtering to documents: {document_ids}")
        except ValueError:
            logger.error("Invalid document IDs format. Use comma-separated integers.")
            sys.exit(1)

    # Initialize RAG Engine
    logger.info("Initializing RAG Engine...")
    rag_engine = RAGEngine(enable_reranking=not args.no_reranking)

    if args.no_reranking:
        logger.info("Reranking disabled")
    else:
        logger.info("Reranking enabled")

    # Extract questions and ground truths
    test_questions = [case['question'] for case in test_cases]
    ground_truths = [case.get('ground_truth', '') for case in test_cases]

    # Run RAG evaluation
    logger.info(f"Running RAG on {len(test_questions)} test questions...")
    evaluation_data = run_rag_evaluation(
        rag_engine=rag_engine,
        test_questions=test_questions,
        ground_truths=ground_truths,
        document_ids=document_ids
    )

    # Add metadata from test cases
    for i, case in enumerate(test_cases):
        if i < len(evaluation_data):
            # Add expected articles if available
            if 'expected_articles' in case:
                evaluation_data[i]['expected_articles'] = case['expected_articles']

            # Add category and difficulty
            if 'category' in case:
                evaluation_data[i]['category'] = case['category']
            if 'difficulty' in case:
                evaluation_data[i]['difficulty'] = case['difficulty']

    # Evaluate with RAGAS (config from settings.py)
    ragas_scores = {}
    if args.llm_provider != 'none':
        logger.info(f"Evaluating with RAGAS metrics (provider: {args.llm_provider})...")

        # Check if API key is configured in settings (reads from env variables)
        if args.llm_provider == 'fpt' and not settings.EVAL_LLM_API_KEY:
            logger.warning("No API key found. Skipping RAGAS evaluation.")
            logger.info("Set environment variable: export EVAL_LLM_API_KEY=your-api-key")
            logger.info("Or: export FPT_API_KEY=your-api-key")
            logger.info("Or: export OPENAI_API_KEY=your-api-key")
        else:
            ragas_scores = evaluate_with_ragas(
                evaluation_data,
                llm_provider=args.llm_provider  # Uses settings from config/settings.py
            )
    else:
        logger.info("Skipping RAGAS evaluation (--llm-provider=none)")

    # Evaluate legal-specific metrics
    logger.info("Evaluating legal-specific metrics...")
    legal_scores = evaluate_legal_metrics(evaluation_data)

    # Print report
    print_evaluation_report(ragas_scores, evaluation_data, legal_scores)

    # Save results
    output_path = args.output
    logger.info(f"Saving results to {output_path}...")

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'num_test_cases': len(test_cases),
                'document_ids': document_ids,
                'reranking_enabled': not args.no_reranking,
                'top_k': args.top_k,
            },
            'ragas_scores': ragas_scores,
            'legal_scores': legal_scores,
            'test_cases': evaluation_data
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… Evaluation complete! Results saved to {output_path}")

    # Print summary
    print("\n" + "="*80)
    print("EVALUATION SUMMARY")
    print("="*80)
    print(f"Test cases: {len(test_cases)}")
    print(f"Document filter: {document_ids if document_ids else 'All documents'}")
    print(f"Reranking: {'Enabled' if not args.no_reranking else 'Disabled'}")
    print(f"Top-K: {args.top_k}")

    if ragas_scores:
        avg_ragas = sum(ragas_scores.values()) / len(ragas_scores)
        print(f"\nğŸ“Š Average RAGAS Score: {avg_ragas:.3f}")

    if legal_scores:
        print(f"ğŸ“‹ Article Citation Accuracy: {legal_scores.get('article_citation_accuracy', 0):.3f}")

    print("="*80 + "\n")


if __name__ == "__main__":
    main()
