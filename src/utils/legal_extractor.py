"""
Legal Document Information Extractor for Vietnamese Legal System
Tr√≠ch xu·∫•t th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam

ENHANCED FOR VIETNAMESE LEGAL DOMAIN:
- C·∫£i thi·ªán metadata extraction (s·ªë hi·ªáu, c∆° quan, ng∆∞·ªùi k√Ω, ng√†y hi·ªáu l·ª±c)
- Nh·∫≠n di·ªán vƒÉn b·∫£n li√™n quan (thay th·∫ø, s·ª≠a ƒë·ªïi, b·ªï sung)
- Tr√≠ch xu·∫•t c·∫•u tr√∫c ph√¢n c·∫•p (Ph·∫ßn > Ch∆∞∆°ng > M·ª•c > ƒêi·ªÅu > Kho·∫£n > ƒêi·ªÉm)
- Legal entity recognition
- Citation format chu·∫©n
"""

import re
from typing import Dict, List, Optional
import logging

logger = logging.getLogger(__name__)


class VietnameseLegalExtractor:
    """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam - Enhanced version"""

    def __init__(self):
        # Regex patterns cho c·∫•u tr√∫c vƒÉn b·∫£n ph√¢n c·∫•p
        self.patterns = {
            'part': re.compile(r'Ph·∫ßn\s+(?:th·ª©\s+)?([IVXLCDM]+|[A-Z]+)', re.IGNORECASE),
            'chapter': re.compile(r'Ch∆∞∆°ng\s+([IVXLCDM]+)', re.IGNORECASE),
            'section': re.compile(r'M·ª•c\s+(\d+)', re.IGNORECASE),
            # ‚úÖ FIXED: ƒêi·ªÅu pattern kh√¥ng y√™u c·∫ßu d·∫•u ch·∫•m
            # Match: "ƒêi·ªÅu X." ho·∫∑c "ƒêi·ªÅu X " ho·∫∑c "ƒêi·ªÅu X\n"
            'article': re.compile(r'ƒêi·ªÅu\s+(\d+)', re.IGNORECASE),
            # üîß FIX: Pattern cho "Kho·∫£n X" v√† s·ªë ƒë·∫ßu d√≤ng "1. ", "2. "
            'clause': re.compile(r'(?:Kho·∫£n\s+(\d+)|^(\d+)\.\s+[A-Z√Ä√Å·∫¢√É·∫†])', re.IGNORECASE | re.MULTILINE),
            'numbered_item': re.compile(r'^(\d+)\.\s+', re.MULTILINE),  # "1. Ho·∫°t ƒë·ªông...", "2. Ch·∫°y t√†u..."
            'point': re.compile(r'[ƒê|ƒë]i·ªÉm\s+([a-z])', re.IGNORECASE),
        }

        # üîß SIMPLIFIED: Ch·ªâ gi·ªØ patterns thi·∫øt y·∫øu
        self.metadata_patterns = {
            # S·ªë hi·ªáu vƒÉn b·∫£n (h·ªó tr·ª£ format "Lu·∫≠t s·ªë: 95/2025/QH15")
            'document_number': re.compile(
                r'(?:Lu·∫≠t|Ngh·ªã ƒë·ªãnh|Quy·∫øt ƒë·ªãnh|Th√¥ng t∆∞|Ngh·ªã quy·∫øt|Ph√°p l·ªánh|Ch·ªâ th·ªã)\s*(?:s·ªë[:\s]*)?(\d+[/-]\d{4}[/-][A-Z\d\-]+)',
                re.IGNORECASE
            ),

            # C∆° quan ban h√†nh
            'issuing_authority': re.compile(
                r'(QU·ªêC\s+H·ªòI|CH√çNH\s+PH·ª¶|TH·ª¶\s+T∆Ø·ªöNG\s+CH√çNH\s+PH·ª¶|'
                r'CH·ª¶\s+T·ªäCH\s+N∆Ø·ªöC|B·ªò\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê\s\-]+)',
                re.IGNORECASE
            ),
        }

        # Ph√¢n lo·∫°i vƒÉn b·∫£n theo th·ª© b·∫≠c hi·ªáu l·ª±c
        self.document_hierarchy = {
            'Hi·∫øn ph√°p': 1,
            'Lu·∫≠t': 2,
            'B·ªô lu·∫≠t': 2,
            'Ph√°p l·ªánh': 3,
            'Ngh·ªã quy·∫øt': 4,
            'Ngh·ªã ƒë·ªãnh': 5,
            'Quy·∫øt ƒë·ªãnh': 6,
            'Th√¥ng t∆∞': 7,
            'Ch·ªâ th·ªã': 8,
            'Quy ƒë·ªãnh': 9,
            'Quy ch·∫ø': 10
        }

        # Legal entities patterns
        self.legal_entities = {
            'organizations': re.compile(
                r'\b(Qu·ªëc\s+h·ªôi|Ch√≠nh\s+ph·ªß|B·ªô\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê\s\-]+|'
                r'·ª¶y\s+ban\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê\s]+|'
                r'T√≤a\s+√°n\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê\s]+)\b',
                re.IGNORECASE
            ),
            'legal_terms': re.compile(
                r'\b(vi\s+ph·∫°m|x·ª≠\s+ph·∫°t|tr√°ch\s+nhi·ªám|quy·ªÅn|nghƒ©a\s+v·ª•|'
                r'khi·∫øu\s+n·∫°i|t·ªë\s+c√°o|tranh\s+ch·∫•p|thi\s+h√†nh|√°p\s+d·ª•ng)\b',
                re.IGNORECASE
            )
        }

    def preprocess_text(self, text: str) -> str:
        """
        üîß NEW: L√†m s·∫°ch vƒÉn b·∫£n tr∆∞·ªõc khi x·ª≠ l√Ω
        - Lo·∫°i b·ªè g·∫°ch d∆∞·ªõi li√™n ti·∫øp (________)
        - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        - Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
        """
        # Lo·∫°i b·ªè g·∫°ch d∆∞·ªõi li√™n ti·∫øp (3 tr·ªü l√™n)
        text = re.sub(r'_{3,}', '', text)

        # Chu·∫©n h√≥a nhi·ªÅu kho·∫£ng tr·∫Øng th√†nh 1
        text = re.sub(r'[ \t]+', ' ', text)

        # Chu·∫©n h√≥a nhi·ªÅu xu·ªëng d√≤ng th√†nh t·ªëi ƒëa 2
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text.strip()

    def extract_structure(self, text: str) -> Dict[str, List[Dict]]:
        """
        Tr√≠ch xu·∫•t c·∫•u tr√∫c ph√¢n c·∫•p ƒë·∫ßy ƒë·ªß c·ªßa vƒÉn b·∫£n (Ph·∫ßn > Ch∆∞∆°ng > M·ª•c > ƒêi·ªÅu > Kho·∫£n > ƒêi·ªÉm)
        üîß IMPROVED: Nh·∫≠n di·ªán c·∫£ "Kho·∫£n X" v√† s·ªë th·ª© t·ª± "1. ", "2. "
        """
        structure = {
            'parts': [],
            'chapters': [],
            'sections': [],
            'articles': [],
            'clauses': [],
            'points': []
        }

        # Tr√≠ch xu·∫•t Ph·∫ßn
        for match in self.patterns['part'].finditer(text):
            structure['parts'].append({
                'number': match.group(1),
                'position': match.start(),
                'text': match.group(0)
            })

        # Tr√≠ch xu·∫•t Ch∆∞∆°ng
        for match in self.patterns['chapter'].finditer(text):
            structure['chapters'].append({
                'number': match.group(1),
                'position': match.start(),
                'text': match.group(0)
            })

        # Tr√≠ch xu·∫•t M·ª•c
        for match in self.patterns['section'].finditer(text):
            structure['sections'].append({
                'number': int(match.group(1)),
                'position': match.start(),
                'text': match.group(0)
            })

        # Tr√≠ch xu·∫•t ƒêi·ªÅu
        for match in self.patterns['article'].finditer(text):
            structure['articles'].append({
                'number': int(match.group(1)),
                'position': match.start(),
                'text': match.group(0)
            })

        # üîß FIX: Tr√≠ch xu·∫•t c·∫£ s·ªë th·ª© t·ª± ƒë·∫ßu d√≤ng (1., 2., ...) trong context c·ªßa ƒêi·ªÅu
        for match in self.patterns['numbered_item'].finditer(text):
            # Ch·ªâ l·∫•y n·∫øu n·∫±m trong m·ªôt ƒêi·ªÅu (kh√¥ng ph·∫£i ·ªü header)
            pos = match.start()
            # Ki·ªÉm tra c√≥ thu·ªôc m·ªôt ƒêi·ªÅu n√†o kh√¥ng
            in_article = False
            for article in structure.get('articles', []):
                if article['position'] < pos:
                    in_article = True
                    break

            if in_article:
                structure['clauses'].append({
                    'number': int(match.group(1)),
                    'position': match.start(),
                    'text': match.group(0)
                })

        # Tr√≠ch xu·∫•t ƒêi·ªÉm
        for match in self.patterns['point'].finditer(text):
            structure['points'].append({
                'letter': match.group(1),
                'position': match.start(),
                'text': match.group(0)
            })

        return structure

    def extract_metadata(self, text: str) -> Dict:
        """
        üîß SIMPLIFIED: Tr√≠ch xu·∫•t metadata thi·∫øt y·∫øu t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam
        Ch·ªâ gi·ªØ l·∫°i 4 fields quan tr·ªçng nh·∫•t
        """
        # L√†m s·∫°ch text tr∆∞·ªõc
        text = self.preprocess_text(text)

        # ‚úÖ CH·ªà GI·ªÆ 4 METADATA THI·∫æT Y·∫æU
        metadata = {
            'document_type': None,        # Lo·∫°i vƒÉn b·∫£n (Lu·∫≠t, Ngh·ªã ƒë·ªãnh, Th√¥ng t∆∞...)
            'document_number': None,      # S·ªë hi·ªáu (VD: 95/2025/QH15)
            'issuing_authority': None,    # C∆° quan ban h√†nh
            'hierarchy_level': None       # C·∫•p ƒë·ªô vƒÉn b·∫£n (1-10)
        }

        # T√¨m ki·∫øm trong ph·∫ßn ƒë·∫ßu vƒÉn b·∫£n
        header_text = text[:2000]

        # 1. X√°c ƒë·ªãnh lo·∫°i vƒÉn b·∫£n v√† c·∫•p ƒë·ªô
        doc_type = self.classify_document_type(text)
        metadata['document_type'] = doc_type
        if doc_type:
            metadata['hierarchy_level'] = self.document_hierarchy.get(doc_type)

        # 2. Tr√≠ch xu·∫•t s·ªë hi·ªáu
        doc_num_match = self.metadata_patterns['document_number'].search(header_text)
        if doc_num_match:
            metadata['document_number'] = doc_num_match.group(1).strip()

        # 3. Tr√≠ch xu·∫•t c∆° quan ban h√†nh
        authority_match = self.metadata_patterns['issuing_authority'].search(header_text)
        if authority_match:
            metadata['issuing_authority'] = authority_match.group(1).strip()

        return metadata

    def classify_document_type(self, text: str) -> Optional[str]:
        """
        üîß IMPROVED: Ph√¢n lo·∫°i d·ª±a tr√™n TITLE LINE, kh√¥ng ph·∫£i to√†n b·ªô header
        Tr√°nh nh·∫ßm l·∫´n khi c√≥ "CƒÉn c·ª© Hi·∫øn ph√°p..." trong header
        """
        # üîß FIX: T√¨m title line, x·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p d√≠nh nhau "LU·∫¨Tƒê∆Ø·ªúNG S·∫ÆT"
        title_pattern = re.compile(
            r'^(HI·∫æN\s*PH√ÅP|B·ªò\s*LU·∫¨T|LU·∫¨T|NGH·ªä\s*ƒê·ªäNH|QUY·∫æT\s*ƒê·ªäNH|TH√îNG\s*T∆Ø|NGH·ªä\s*QUY·∫æT|PH√ÅP\s*L·ªÜNH|CH·ªà\s*TH·ªä|QUY\s*ƒê·ªäNH|QUY\s*CH·∫æ)(?:[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê\s]*)?$',
            re.IGNORECASE | re.MULTILINE
        )

        # T√¨m trong 1500 k√Ω t·ª± ƒë·∫ßu
        for line in text[:1500].split('\n'):
            line_clean = line.strip()
            match = title_pattern.match(line_clean)
            if match:
                doc_type_upper = match.group(1).upper()
                # Map v·ªÅ format chu·∫©n
                for doc_type, _ in self.document_hierarchy.items():
                    if doc_type.upper() == doc_type_upper:
                        logger.info(f"Detected document type: {doc_type} from title line: {line_clean[:50]}")
                        return doc_type

        # Fallback: t√¨m theo c√°ch c≈© nh∆∞ng ∆∞u ti√™n th·∫•p h∆°n
        text_sample = text[:1000].upper()
        sorted_types = sorted(
            self.document_hierarchy.items(),
            key=lambda x: x[1],
            reverse=True  # ∆Øu ti√™n t·ª´ th·∫•p ƒë·∫øn cao ƒë·ªÉ tr√°nh "Hi·∫øn ph√°p" trong "CƒÉn c·ª©"
        )

        for doc_type, _ in sorted_types:
            if doc_type.upper() in text_sample:
                logger.warning(f"Fallback detection for document type: {doc_type}")
                return doc_type

        return None

    def extract_legal_entities(self, text: str) -> Dict[str, List[str]]:
        """Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ ph√°p l√Ω (c∆° quan, thu·∫≠t ng·ªØ ph√°p l√Ω)"""
        entities = {
            'organizations': [],
            'legal_terms': []
        }

        # Tr√≠ch xu·∫•t t√™n t·ªï ch·ª©c/c∆° quan
        org_matches = self.legal_entities['organizations'].finditer(text)
        seen_orgs = set()
        for match in org_matches:
            org = match.group(1).strip()
            if org not in seen_orgs:
                entities['organizations'].append(org)
                seen_orgs.add(org)

        # Tr√≠ch xu·∫•t thu·∫≠t ng·ªØ ph√°p l√Ω
        term_matches = self.legal_entities['legal_terms'].finditer(text)
        seen_terms = set()
        for match in term_matches:
            term = match.group(1).strip()
            if term not in seen_terms:
                entities['legal_terms'].append(term)
                seen_terms.add(term)

        return entities

    def create_citation(self, metadata: Dict, article: Optional[int] = None,
                       clause: Optional[int] = None, point: Optional[str] = None) -> str:
        """
        T·∫°o citation chu·∫©n cho vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam

        Format: Theo [T√™n vƒÉn b·∫£n] [S·ªë hi·ªáu], ƒêi·ªÅu X, Kho·∫£n Y, ƒêi·ªÉm z
        VD: Theo B·ªô lu·∫≠t D√¢n s·ª± s·ªë 91/2015/QH13, ƒêi·ªÅu 1, Kho·∫£n 1
        """
        parts = []

        # Lo·∫°i vƒÉn b·∫£n
        if metadata.get('document_type'):
            parts.append(metadata['document_type'])

        # S·ªë hi·ªáu
        if metadata.get('document_number'):
            parts.append(f"s·ªë {metadata['document_number']}")

        citation = "Theo " + " ".join(parts) if parts else "Theo vƒÉn b·∫£n"

        # Th√™m ƒêi·ªÅu, Kho·∫£n, ƒêi·ªÉm n·∫øu c√≥
        if article:
            citation += f", ƒêi·ªÅu {article}"
        if clause:
            citation += f", Kho·∫£n {clause}"
        if point:
            citation += f", ƒêi·ªÉm {point}"

        return citation

    def split_by_legal_structure(self, text: str, max_chunk_size: int = 1200,
                                 overlap: int = 150) -> List[Dict]:
        """
        Chia vƒÉn b·∫£n theo c·∫•u tr√∫c ph√°p l√Ω ph√¢n c·∫•p
        ∆Øu ti√™n: ƒêi·ªÅu > Kho·∫£n > Ph√¢n ƒëo·∫°n t·ª± nhi√™n
        """
        structure = self.extract_structure(text)
        chunks = []

        if not structure['articles']:
            # Kh√¥ng t√¨m th·∫•y ƒêi·ªÅu, chia theo c√°ch th√¥ng th∆∞·ªùng
            return self._split_generic(text, max_chunk_size, overlap)

        # Chia theo t·ª´ng ƒêi·ªÅu
        articles = structure['articles']
        for i, article in enumerate(articles):
            start = article['position']
            end = articles[i + 1]['position'] if i + 1 < len(articles) else len(text)

            article_content = text[start:end].strip()

            # T√¨m chapter/section ch·ª©a ƒëi·ªÅu n√†y (n·∫øu c√≥)
            parent_chapter = self._find_parent_structure(
                article['position'],
                structure['chapters']
            )
            parent_section = self._find_parent_structure(
                article['position'],
                structure['sections']
            )

            chunk_metadata = {
                'article': article['number'],
                'chapter': parent_chapter['number'] if parent_chapter else None,
                'section': parent_section['number'] if parent_section else None,
                'type': 'article'
            }

            if len(article_content) <= max_chunk_size:
                # ƒêi·ªÅu v·ª´a ƒë·ªß, l∆∞u nguy√™n
                # ‚úÖ FIX: Only add if content is meaningful (>= 100 chars)
                if len(article_content) >= 100:
                    chunks.append({
                        'content': article_content,
                        **chunk_metadata
                    })
                else:
                    logger.warning(f"Skipping article {chunk_metadata.get('article')} - too small ({len(article_content)} bytes)")
            else:
                # ƒêi·ªÅu qu√° d√†i, c·∫ßn chia nh·ªè theo Kho·∫£n ho·∫∑c generic
                sub_chunks = self._split_long_article(
                    article_content,
                    max_chunk_size,
                    overlap,
                    start_position=start
                )
                for chunk in sub_chunks:
                    chunk.update(chunk_metadata)
                chunks.extend(sub_chunks)

        return chunks

    def _find_parent_structure(self, position: int, parent_list: List[Dict]) -> Optional[Dict]:
        """T√¨m c·∫•u tr√∫c cha (chapter/section) ch·ª©a v·ªã tr√≠ n√†y"""
        for i, parent in enumerate(parent_list):
            next_pos = parent_list[i + 1]['position'] if i + 1 < len(parent_list) else float('inf')
            if parent['position'] <= position < next_pos:
                return parent
        return None

    def _split_long_article(self, content: str, max_size: int, overlap: int,
                           start_position: int) -> List[Dict]:
        """
        üîß IMPROVED: Chia ƒêi·ªÅu d√†i th√†nh c√°c chunks nh·ªè h∆°n
        ∆Øu ti√™n: s·ªë th·ª© t·ª± "1. ", "2. " > "Kho·∫£n X" > generic split
        """
        chunks = []

        # üîß FIX: T√¨m c√°c kho·∫£n theo s·ªë th·ª© t·ª± ƒë·∫ßu d√≤ng "1. ", "2. ", ...
        numbered_pattern = re.compile(r'^(\d+)\.\s+', re.MULTILINE)
        numbered_items = list(numbered_pattern.finditer(content))

        # Fallback: t√¨m "Kho·∫£n X"
        clause_pattern = re.compile(r'Kho·∫£n\s+\d+', re.IGNORECASE)
        clauses = list(clause_pattern.finditer(content))

        # Ch·ªçn pattern ph√π h·ª£p nh·∫•t (∆∞u ti√™n numbered_items n·∫øu nhi·ªÅu h∆°n)
        if len(numbered_items) > len(clauses):
            items_to_split = numbered_items
            split_type = 'numbered_clause'
        else:
            items_to_split = clauses
            split_type = 'clause'

        if not items_to_split or len(content) < max_size * 1.5:
            # Kh√¥ng c√≥ kho·∫£n ho·∫∑c kh√¥ng qu√° d√†i, chia generic
            return self._split_generic(content, max_size, overlap)

        # Chia theo c√°c kho·∫£n t√¨m ƒë∆∞·ª£c
        for i, item_match in enumerate(items_to_split):
            item_start = item_match.start()
            item_end = items_to_split[i + 1].start() if i + 1 < len(items_to_split) else len(content)

            item_content = content[item_start:item_end].strip()

            if len(item_content) <= max_size:
                # ‚úÖ FIX: Only add if content is meaningful (>= 100 chars)
                if len(item_content) >= 100:
                    chunks.append({
                        'content': item_content,
                        'type': split_type
                    })
                else:
                    logger.debug(f"Skipping {split_type} - too small ({len(item_content)} bytes)")
            else:
                # Kho·∫£n v·∫´n qu√° d√†i, chia generic
                sub_chunks = self._split_generic(item_content, max_size, overlap)
                chunks.extend(sub_chunks)

        return chunks

    def _split_generic(self, text: str, max_size: int, overlap: int) -> List[Dict]:
        """
        üîß FIXED: Chia vƒÉn b·∫£n theo c√°ch th√¥ng th∆∞·ªùng v·ªõi overlap
        Fix v√≤ng l·∫∑p v√¥ h·∫°n khi overlap > 0
        """
        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            end = min(start + max_size, text_len)

            # T√¨m ƒëi·ªÉm ng·∫Øt t·ª± nhi√™n g·∫ßn end
            if end < text_len:
                # T√¨m d·∫•u xu·ªëng d√≤ng, ch·∫•m, ho·∫∑c d·∫•u ph·∫©y g·∫ßn nh·∫•t
                natural_breaks = [
                    text.rfind('\n\n', start, end),
                    text.rfind('\n', start, end),
                    text.rfind('. ', start, end),
                    text.rfind('; ', start, end),
                ]
                best_break = max(b for b in natural_breaks if b > start)
                if best_break > start + max_size * 0.5:  # Gi·∫£m threshold t·ª´ 0.7 -> 0.5
                    end = best_break + 1

            chunk_content = text[start:end].strip()

            # ‚úÖ FIX: Skip chunks that are too small (likely incomplete references like "kho·∫£n 3 ƒêi·ªÅu 148")
            # Minimum 100 characters to ensure meaningful content
            if len(chunk_content) < 100:
                logger.warning(f"Skipping chunk too small ({len(chunk_content)} bytes): {chunk_content[:50]}...")
            elif chunk_content:
                chunks.append({
                    'content': chunk_content,
                    'type': 'generic'
                })

            # üîß FIX: ƒê·∫£m b·∫£o start lu√¥n ti·∫øn v·ªÅ ph√≠a tr∆∞·ªõc
            if end >= text_len:
                # ƒê√£ ƒë·∫øn cu·ªëi vƒÉn b·∫£n
                break
            else:
                # Di chuy·ªÉn start v·ªõi overlap, nh∆∞ng ƒë·∫£m b·∫£o lu√¥n ti·∫øn
                new_start = end - overlap
                if new_start <= start:
                    # N·∫øu overlap qu√° l·ªõn khi·∫øn kh√¥ng ti·∫øn, force di chuy·ªÉn
                    new_start = start + max(1, max_size // 2)
                start = new_start

        return chunks
